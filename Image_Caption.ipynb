{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "mount_file_id": "1mJmKftn4wG73oU1V8V9vNGIimyrEOXK0",
      "authorship_tag": "ABX9TyOqy1ACS01P7TQiWAQrdSUO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SajlaKM/ImageCaption/blob/main/Image_Caption.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "from transformers import ViTModel, ViTFeatureExtractor, GPT2Tokenizer, GPT2LMHeadModel\n",
        "from PIL import Image\n",
        "import zipfile\n",
        "import os\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "lYcJMFcdA5ov",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bab7e96-3159-4bbf-f22d-1663c55945c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch_xla/__init__.py:253: UserWarning: `tensorflow` can conflict with `torch-xla`. Prefer `tensorflow-cpu` when using PyTorch/XLA. To silence this warning, `pip uninstall -y tensorflow && pip install tensorflow-cpu`. If you are in a notebook environment such as Colab or Kaggle, restart your notebook runtime afterwards.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "DATASET_PATH = \"/content/drive/MyDrive/Image caption dataset /imagecaptiondata.zip\"\n",
        "extract_path = \"/content/dataset\"\n",
        "with zipfile.ZipFile(DATASET_PATH, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "print(\"Extracted files:\", os.listdir(extract_path))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJcLdwuAAz8D",
        "outputId": "4d553dee-3551-4a9c-bbbb-b07c19b5225c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Extracted files: ['Images', 'captions.txt']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "from transformers import ViTModel, GPT2Tokenizer, GPT2LMHeadModel\n",
        "from PIL import Image\n",
        "import zipfile\n",
        "import os\n",
        "import glob\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "# Update dataset path to point to the correct subdirectory\n",
        "IMAGE_DIR = os.path.join(extract_path, \"Images\")  # Updated path\n",
        "\n",
        "# Check extracted files and subdirectories\n",
        "print(\"Extracted directory structure:\")\n",
        "for root, dirs, files in os.walk(extract_path):\n",
        "    print(f\"ðŸ“‚ {root}\")\n",
        "    for file in files[:5]:  # Show first 5 files in each folder\n",
        "        print(f\"   ðŸ“„ {file}\")\n",
        "\n",
        "# Ensure the directory has images\n",
        "if not os.path.exists(IMAGE_DIR) or len(os.listdir(IMAGE_DIR)) == 0:\n",
        "    raise ValueError(f\"No images found in {IMAGE_DIR}. Check your dataset extraction.\")\n",
        "\n",
        "# Fine-tuned Vision Transformer (ViT) for feature extraction\n",
        "class FineTunedViT(nn.Module):\n",
        "    def __init__(self, vit_model_name=\"google/vit-base-patch16-224\"):\n",
        "        super(FineTunedViT, self).__init__()\n",
        "        self.vit = ViTModel.from_pretrained(vit_model_name)\n",
        "\n",
        "        # Remove the uninitialized pooling layer\n",
        "        self.vit.pooler = None\n",
        "\n",
        "        # Add a custom classifier for fine-tuning\n",
        "        self.fc = nn.Linear(self.vit.config.hidden_size, 256)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.vit(x).last_hidden_state[:, 0, :]\n",
        "        return self.fc(features)\n",
        "\n",
        "# CPTR Model: Fine-tuned ViT as encoder, Transformer decoder\n",
        "class CPTR(nn.Module):\n",
        "    def __init__(self, vit_model_name=\"google/vit-base-patch16-224\", gpt2_model_name=\"gpt2\"):\n",
        "        super(CPTR, self).__init__()\n",
        "        self.vit = FineTunedViT(vit_model_name)\n",
        "        self.tokenizer = GPT2Tokenizer.from_pretrained(gpt2_model_name)\n",
        "        self.decoder = GPT2LMHeadModel.from_pretrained(gpt2_model_name)\n",
        "\n",
        "        # Linear layer to match encoder-decoder dimensions\n",
        "        self.linear = nn.Linear(256, self.decoder.config.n_embd)\n",
        "\n",
        "    def forward(self, image, captions):\n",
        "        vit_features = self.vit(image)\n",
        "        vit_features = self.linear(vit_features).unsqueeze(1)\n",
        "\n",
        "        caption_inputs = self.tokenizer(captions, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "        input_ids = caption_inputs.input_ids\n",
        "\n",
        "        outputs = self.decoder(input_ids, encoder_hidden_states=vit_features)\n",
        "        return outputs.logits\n",
        "\n",
        "# Image preprocessing\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "# Define Custom Dataset for Image Captioning\n",
        "class ImageCaptionDataset(Dataset):\n",
        "    def __init__(self, image_dir, transform):\n",
        "        self.image_paths = glob.glob(os.path.join(image_dir, \"*.jpg\")) + glob.glob(os.path.join(image_dir, \"*.png\"))\n",
        "\n",
        "        if len(self.image_paths) == 0:\n",
        "            print(\"No images found! Checking available file types...\")\n",
        "            print(\"Files:\", os.listdir(image_dir))  # Debugging: Show all files\n",
        "            raise ValueError(\"No image files found in the dataset directory!\")\n",
        "\n",
        "        print(f\"Found {len(self.image_paths)} images.\")  # Print found image count\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = self.image_paths[idx]\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "        image = self.transform(image)\n",
        "        return image, image_path  # Labels (captions) are not provided here\n",
        "\n",
        "# Create DataLoader\n",
        "dataset = ImageCaptionDataset(IMAGE_DIR, transform)\n",
        "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
        "\n",
        "# Load CPTR model with fine-tuned ViT\n",
        "model = CPTR()\n",
        "\n",
        "# Fine-tune the ViT model\n",
        "optimizer = torch.optim.Adam(model.vit.parameters(), lr=1e-4)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training Loop (Now Using Defined DataLoader)\n",
        "for epoch in range(5):  # Adjust based on dataset size\n",
        "    for images, image_paths in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model.vit(images)\n",
        "        loss = loss_fn(outputs, torch.zeros_like(outputs))  # Dummy loss as captions are not available\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Load and preprocess image\n",
        "def process_image(image_path):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    return transform(image).unsqueeze(0)\n",
        "\n",
        "# Generate caption\n",
        "def generate_caption(model, image_path, max_length=20):\n",
        "    model.eval()\n",
        "    image = process_image(image_path)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        vit_features = model.vit(image)\n",
        "        vit_features = model.linear(vit_features).unsqueeze(1)\n",
        "\n",
        "        generated_ids = model.decoder.generate(\n",
        "            encoder_hidden_states=vit_features,\n",
        "            max_length=max_length,\n",
        "            pad_token_id=model.tokenizer.eos_token_id\n",
        "        )\n",
        "        caption = model.tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "    return caption\n",
        "\n",
        "# Generate captions for images in the dataset\n",
        "dataset_images = [os.path.join(IMAGE_DIR, img) for img in os.listdir(IMAGE_DIR) if img.endswith(('.jpg', '.png'))]\n",
        "for img_path in dataset_images[:5]:  # Show captions for first 5 images\n",
        "    caption = generate_caption(model, img_path)\n",
        "    print(f\"Image: {img_path} \\nGenerated Caption: {caption}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JYm-11myOa5T",
        "outputId": "cbc1864f-239d-406f-ed99-29986320c027"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted directory structure:\n",
            "ðŸ“‚ /content/dataset\n",
            "   ðŸ“„ captions.txt\n",
            "ðŸ“‚ /content/dataset/Images\n",
            "   ðŸ“„ 649596742_5ba84ce946.jpg\n",
            "   ðŸ“„ 854333409_38bc1da9dc.jpg\n",
            "   ðŸ“„ 1087168168_70280d024a.jpg\n",
            "   ðŸ“„ 3028969146_26929ae0e8.jpg\n",
            "   ðŸ“„ 3385956569_a849218e34.jpg\n",
            "Found 8091 images.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    }
  ]
}